//! üúÇ –£—Ä–æ–≤–µ–Ω—å 1: –ñ—Ä–µ—Ü—ã –ñ–µ–ª–µ–∑–∞ - –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–≤–∏–∂–æ–∫
//!
//! –í—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω—ã–π –¥–≤–∏–∂–æ–∫ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –Ω–∞ –±–∞–∑–µ intfloat/multilingual-e5-small
//! –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è RTX 4090 32GB —Å –±–∞—Ç—á–∏–Ω–≥–æ–º –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º

#![allow(dead_code)]

use anyhow::{anyhow, Result};
use candle_core::{DType, Device, Tensor};
use candle_nn::VarBuilder;
use candle_transformers::models::bert::{BertModel, Config};
use parking_lot::RwLock;
use std::collections::HashMap;
use std::sync::Arc;
use tokenizers::Tokenizer;

/// Trait –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–Ω—ã–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏
pub trait Embedder: Send + Sync {
    fn embed(&self, text: &str) -> Result<Vec<f32>>;
    fn embedding_dim(&self) -> usize;
}

/// –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–≤–∏–∂–∫–∞
#[derive(Debug, Clone)]
pub struct EmbeddingConfig {
    /// –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ (384 –¥–ª—è e5-small)
    pub embedding_dim: usize,
    /// –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    pub max_length: usize,
    /// –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ GPU
    pub batch_size: usize,
    /// –†–∞–∑–º–µ—Ä –∫—ç—à–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    pub cache_size: usize,
    /// –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –ª–∏ –≤–µ–∫—Ç–æ—Ä—ã (cosine similarity)
    pub normalize: bool,
}

impl Default for EmbeddingConfig {
    fn default() -> Self {
        Self {
            embedding_dim: 384, // e5-small
            max_length: 512,
            batch_size: 32,   // –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è RTX 4090
            cache_size: 1000, // –ö—ç—à –¥–ª—è —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
            normalize: true,  // –î–ª—è cosine similarity
        }
    }
}

/// –í—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–≤–∏–∂–æ–∫
pub struct EmbeddingEngine {
    /// BERT –º–æ–¥–µ–ª—å –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
    model: BertModel,
    /// –¢–æ–∫–µ–Ω–∞–π–∑–µ—Ä –¥–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞
    tokenizer: Tokenizer,
    /// –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π (GPU/CPU)
    device: Device,
    /// –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–≤–∏–∂–∫–∞
    config: EmbeddingConfig,
    /// –ö—ç—à –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    cache: Arc<RwLock<HashMap<String, Vec<f32>>>>,
    /// –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    stats: Arc<RwLock<EmbeddingStats>>,
}

/// –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–≤–∏–∂–∫–∞
#[derive(Debug, Default, Clone)]
pub struct EmbeddingStats {
    pub total_requests: usize,
    pub cache_hits: usize,
    pub cache_misses: usize,
    pub total_tokens_processed: usize,
    pub avg_batch_size: f32,
}

impl EmbeddingEngine {
    /// –°–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–≤–∏–∂–æ–∫
    pub fn new(model_path: &str, device: Device) -> Result<Self> {
        let config = EmbeddingConfig::default();
        Self::with_config(model_path, device, config)
    }

    /// –°–æ–∑–¥–∞–µ—Ç –¥–≤–∏–∂–æ–∫ —Å –∫–∞—Å—Ç–æ–º–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
    pub fn with_config(model_path: &str, device: Device, config: EmbeddingConfig) -> Result<Self> {
        println!("üß† –ó–∞–≥—Ä—É–∑–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥ –º–æ–¥–µ–ª–∏: {}", model_path);

        // –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏
        let config_path = std::path::Path::new(model_path).join("config.json");
        let config_content = std::fs::read_to_string(config_path)?;
        let model_config: Config = serde_json::from_str(&config_content)?;

        // –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏
        let weights_path = std::path::Path::new(model_path).join("model.safetensors");
        let vb =
            unsafe { VarBuilder::from_mmaped_safetensors(&[&weights_path], DType::F32, &device)? };
        let model = BertModel::load(vb, &model_config)?;

        // –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞
        let tokenizer_path = std::path::Path::new(model_path).join("tokenizer.json");
        let tokenizer = Tokenizer::from_file(tokenizer_path)
            .map_err(|e| anyhow!("Failed to load tokenizer: {}", e))?;

        println!(
            "‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–≤–∏–∂–æ–∫ –∑–∞–≥—Ä—É–∂–µ–Ω (dim: {})",
            config.embedding_dim
        );

        Ok(Self {
            model,
            tokenizer,
            device,
            config,
            cache: Arc::new(RwLock::new(HashMap::new())),
            stats: Arc::new(RwLock::new(EmbeddingStats::default())),
        })
    }

    /// –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ—Ç –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç
    pub fn embed(&self, text: &str) -> Result<Vec<f32>> {
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        if let Some(embedding) = self.get_from_cache(text) {
            self.update_stats(true, 0);
            return Ok(embedding);
        }

        // –í—ã—á–∏—Å–ª—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
        let embedding = self.compute_embedding(text)?;

        // –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
        self.add_to_cache(text.to_string(), embedding.clone());
        self.update_stats(false, 1);

        Ok(embedding)
    }

    /// –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ—Ç –±–∞—Ç—á —Ç–µ–∫—Å—Ç–æ–≤ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π GPU
    pub fn embed_batch(&self, texts: &[String]) -> Result<Vec<Vec<f32>>> {
        if texts.is_empty() {
            return Ok(vec![]);
        }

        // –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        let mut results: Vec<Vec<f32>> = Vec::with_capacity(texts.len());
        let mut uncached_texts: Vec<(usize, String)> = Vec::new();

        for (i, text) in texts.iter().enumerate() {
            if let Some(embedding) = self.get_from_cache(text) {
                results.push(embedding);
                self.update_stats(true, 0);
            } else {
                results.push(Vec::new()); // –ó–∞–ø–æ–ª–Ω–∏—Ç–µ–ª—å
                uncached_texts.push((i, text.clone()));
            }
        }

        // –í—ã—á–∏—Å–ª—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –Ω–µ–∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –±–∞—Ç—á–∞–º–∏
        if !uncached_texts.is_empty() {
            let batch_embeddings = self.compute_batch_embeddings(
                &uncached_texts
                    .iter()
                    .map(|(_, t)| t.as_str())
                    .collect::<Vec<_>>(),
            )?;

            // –û–±–Ω–æ–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –∫—ç—à
            for ((idx, text), embedding) in uncached_texts.iter().zip(batch_embeddings.iter()) {
                results[*idx] = embedding.clone();
                self.add_to_cache(text.clone(), embedding.clone());
            }

            self.update_stats(false, uncached_texts.len());
        }

        Ok(results)
    }

    /// –í—ã—á–∏—Å–ª—è–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    fn compute_embedding(&self, text: &str) -> Result<Vec<f32>> {
        // –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è e5-small
        let processed_text = format!("query: {}", text);

        // –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        let tokens = self
            .tokenizer
            .encode(processed_text.as_str(), true)
            .map_err(|e| anyhow!("Tokenization failed: {}", e))?;

        // –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–Ω–∑–æ—Ä–æ–≤ (2D: batch_size=1, seq_len)
        let token_ids = Tensor::new(tokens.get_ids(), &self.device)?.unsqueeze(0)?;
        let attention_mask =
            Tensor::new(tokens.get_attention_mask(), &self.device)?.unsqueeze(0)?;

        // Forward pass
        let output = self.model.forward(&token_ids, &attention_mask, None)?;

        // Mean pooling –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
        let pooled = output.mean(1)?.squeeze(0)?;
        let embedding = if self.config.normalize {
            self.l2_normalize(&pooled.to_vec1()?)?
        } else {
            pooled.to_vec1()?
        };

        Ok(embedding)
    }

    /// –í—ã—á–∏—Å–ª—è–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –±–∞—Ç—á–∞ —Ç–µ–∫—Å—Ç–æ–≤
    fn compute_batch_embeddings(&self, texts: &[&str]) -> Result<Vec<Vec<f32>>> {
        let mut embeddings = Vec::with_capacity(texts.len());

        // –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á–∞–º–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ GPU
        for chunk in texts.chunks(self.config.batch_size) {
            // –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è e5-small
            let processed_texts: Vec<String> =
                chunk.iter().map(|t| format!("query: {}", t)).collect();

            // –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –±–∞—Ç—á–∞
            let mut all_token_ids: Vec<u32> = Vec::new();
            let mut all_attention_masks: Vec<u32> = Vec::new();

            for text in &processed_texts {
                let tokens = self
                    .tokenizer
                    .encode(text.as_str(), true)
                    .map_err(|e| anyhow!("Tokenization failed: {}", e))?;

                all_token_ids.extend(tokens.get_ids());
                all_attention_masks.extend(tokens.get_attention_mask());
            }

            // –°–æ–∑–¥–∞–µ–º —Ç–µ–Ω–∑–æ—Ä—ã –±–∞—Ç—á–∞
            let batch_size = chunk.len();
            let seq_len = all_token_ids.len() / batch_size;

            let token_ids = Tensor::from_vec(all_token_ids, (batch_size, seq_len), &self.device)?;
            let attention_mask =
                Tensor::from_vec(all_attention_masks, (batch_size, seq_len), &self.device)?;

            // Forward pass
            let output = self.model.forward(&token_ids, &attention_mask, None)?;

            // Mean pooling –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ –±–∞—Ç—á–∞
            for i in 0..batch_size {
                let pooled = output.get(i)?.mean(0)?;
                let embedding = if self.config.normalize {
                    self.l2_normalize(&pooled.to_vec1()?)?
                } else {
                    pooled.to_vec1()?
                };
                embeddings.push(embedding);
            }
        }

        Ok(embeddings)
    }

    /// L2 –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–∞
    fn l2_normalize(&self, vec: &[f32]) -> Result<Vec<f32>> {
        let norm: f32 = vec.iter().map(|x| x * x).sum::<f32>().sqrt();
        if norm == 0.0 {
            return Ok(vec.to_vec());
        }
        Ok(vec.iter().map(|x| x / norm).collect())
    }

    /// –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫—ç—à –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ –µ—Å–ª–∏ –Ω–∞–π–¥–µ–Ω
    fn get_from_cache(&self, text: &str) -> Option<Vec<f32>> {
        let cache = self.cache.read();
        cache.get(text).cloned()
    }

    /// –î–æ–±–∞–≤–ª—è–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ –≤ –∫—ç—à —Å LRU eviction
    fn add_to_cache(&self, text: String, embedding: Vec<f32>) {
        let mut cache = self.cache.write();

        // –ï—Å–ª–∏ –∫—ç—à –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω, —É–¥–∞–ª—è–µ–º —Å–∞–º—ã–µ —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏
        if cache.len() >= self.config.cache_size {
            // –ü—Ä–æ—Å—Ç–∞—è LRU —Å—Ç—Ä–∞—Ç–µ–≥–∏—è - —É–¥–∞–ª—è–µ–º –ø–µ—Ä–≤—ã–µ 20% –∑–∞–ø–∏—Å–µ–π
            let remove_count = self.config.cache_size / 5;
            let keys_to_remove: Vec<String> = cache.keys().take(remove_count).cloned().collect();

            for key in keys_to_remove {
                cache.remove(&key);
            }
        }

        cache.insert(text, embedding);
    }

    /// –û–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    fn update_stats(&self, cache_hit: bool, tokens_processed: usize) {
        let mut stats = self.stats.write();
        stats.total_requests += 1;

        if cache_hit {
            stats.cache_hits += 1;
        } else {
            stats.cache_misses += 1;
            stats.total_tokens_processed += tokens_processed;
        }
    }

    /// –í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –¥–≤—É–º—è –≤–µ–∫—Ç–æ—Ä–∞–º–∏
    pub fn cosine_similarity(&self, a: &[f32], b: &[f32]) -> Result<f32> {
        if a.len() != b.len() {
            return Err(anyhow!(
                "Vector dimensions don't match: {} vs {}",
                a.len(),
                b.len()
            ));
        }

        let dot_product: f32 = a.iter().zip(b).map(|(x, y)| x * y).sum();
        let norm_a: f32 = a.iter().map(|x| x * x).sum::<f32>().sqrt();
        let norm_b: f32 = b.iter().map(|x| x * x).sum::<f32>().sqrt();

        if norm_a == 0.0 || norm_b == 0.0 {
            return Ok(0.0);
        }

        Ok(dot_product / (norm_a * norm_b))
    }

    /// –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    pub fn get_stats(&self) -> EmbeddingStats {
        self.stats.read().clone()
    }

    /// –û—á–∏—â–∞–µ—Ç –∫—ç—à
    pub fn clear_cache(&self) {
        self.cache.write().clear();
    }

    /// –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞–∑–º–µ—Ä –∫—ç—à–∞
    pub fn cache_size(&self) -> usize {
        self.cache.read().len()
    }

    /// –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
    pub fn embedding_dim(&self) -> usize {
        self.config.embedding_dim
    }
}

impl Embedder for EmbeddingEngine {
    fn embed(&self, text: &str) -> Result<Vec<f32>> {
        self.embed(text)
    }

    fn embedding_dim(&self) -> usize {
        self.embedding_dim()
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_embedding_config_default() {
        let config = EmbeddingConfig::default();
        assert_eq!(config.embedding_dim, 384);
        assert_eq!(config.max_length, 512);
        assert_eq!(config.batch_size, 32);
        assert!(config.normalize);
    }

    #[test]
    fn test_cosine_similarity() {
        let engine = EmbeddingEngine::new("dummy_path", Device::Cpu);

        let a = vec![1.0, 0.0, 0.0];
        let b = vec![0.0, 1.0, 0.0];
        let c = vec![1.0, 0.0, 0.0];

        assert_eq!(engine.cosine_similarity(&a, &b).unwrap(), 0.0);
        assert_eq!(engine.cosine_similarity(&a, &c).unwrap(), 1.0);
    }
}
